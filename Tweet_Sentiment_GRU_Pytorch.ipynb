{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport spacy\nimport string\nimport re\nimport numpy as np\nfrom spacy.symbols import ORTH\nfrom collections import Counter\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntrain_df['text'] = train_df['text'].astype(str)\ntrain_df['selected_text'] = train_df['selected_text'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"re_br = re.compile(r'<\\s*br\\s*/?>', re.IGNORECASE)\ndef sub_br(x): return re_br.sub(\"\\n\", x)\n\nmy_tok = spacy.load('en')\ndef spacy_tok(x): return [tok.text for tok in my_tok.tokenizer(x)]\n\ndef split_text(x): return x.split(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize_text(x):\n    return split_text(x['text'])\n#     return [tok.text for tok in my_tok.tokenizer(x['text'])]\ndef tokenize_selected_text(x):\n    return split_text(x['selected_text'])\n#     return [tok.text for tok in my_tok.tokenizer(x['selected_text'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['tokenized_text'] = train_df.apply(tokenize_text, axis=1)\ntrain_df['tokenized_selected_text'] = train_df.apply(tokenize_selected_text, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_starting_index(x):\n    tokenized_text = x['tokenized_text']\n    tokenized_selected_text = x['tokenized_selected_text']\n    for i in range(len(tokenized_text)):\n        if tokenized_text[i] == tokenized_selected_text[0] and \\\n           tokenized_text[i:i+len(tokenized_selected_text)]==tokenized_selected_text:\n            break\n    return i\ndef get_ending_index(x):\n    return x['starting_idx']+len(x['tokenized_selected_text'])\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['starting_idx'] = train_df.apply(get_starting_index, axis=1)\ntrain_df['ending_idx'] = train_df.apply(get_ending_index, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_df['tokenized_text'].to_numpy()\nstarting_idx, ending_idx = train_df['starting_idx'].to_list(), train_df['ending_idx'].to_list()\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_start_train, y_start_val, y_end_train, y_end_val = train_test_split(X, starting_idx, ending_idx, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for t in X:\n    if 'Doctor' in t:\n        print(t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = Counter()\nfor text in X_train:\n    counts.update(text)\nlen(counts.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for word in list(counts):\n    if counts[word] < 5:\n        del counts[word]\nlen(counts.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab2index = {\"\":0, \"UNK\":1}\nwords = [\"\", \"UNK\"]\nfor word in counts:\n    vocab2index[word] = len(words)\n    words.append(word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_len = [len(t) for t in X_train]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.percentile(X_train_len, 95) # let set the max sequence len to N=40","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_sentence(x, vocab2index, N=30, padding_start=True):\n    enc = np.zeros(N, dtype=np.int32)\n    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in x])\n    l = min(N, len(enc1))\n    if padding_start:\n        enc[:l] = enc1[:l]\n    else:\n        enc[N-l:] = enc1[:l]\n    return enc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset(Dataset):\n    def __init__(self, X, y_start, y_end, N=30, padding_start=True):\n        self.X = [encode_sentence(t, vocab2index, N, padding_start) for t in X]\n        self.y_start = y_start\n        self.y_end = y_end\n        \n    def __len__(self):\n        return len(self.y_start)\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.y_start[idx], self.y_end[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds = TweetDataset(X_train, y_start_train, y_end_train)\nval_ds = TweetDataset(X_val, y_start_val, y_end_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 10\ntrain_dl = DataLoader(train_ds, batch_size=batch_size)\nval_dl = DataLoader(val_ds, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x, y1, y2 = next(iter(train_dl))\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    start_loss = F.mse_loss(start_logits, start_positions)\n    end_loss = F.mse_loss(end_logits, end_positions)    \n    total_loss = start_loss + end_loss\n    return total_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetGRU(torch.nn.Module) :\n    def __init__(self, vocab_size, embedding_dim=50, hidden_dim=50) :\n        super(TweetGRU,self).__init__()\n        self.hidden_dim = hidden_dim\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n        self.dropout = nn.Dropout(0.5)\n        self.linear = nn.Linear(hidden_dim, 2)\n        \n    def forward(self, x):\n        x = self.embeddings(x)\n        x = self.dropout(x)\n        _, ht = self.gru(x)\n        x = self.linear(ht[-1])\n        start_logits, end_logits = x.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n                \n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_optimizer(optimizer, lr):\n    for i, param_group in enumerate(optimizer.param_groups):\n        param_group[\"lr\"] = lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epocs(model, optimizer, train_dl, val_dl, epochs=10):\n    for i in range(epochs):\n        model.train()\n        sum_loss = 0.0\n        total = 0\n        for x, y_start, y_end in train_dl:\n            x = x.long().cuda()\n            y_start = y_start.float().cuda()\n            y_end = y_end.float().cuda()\n            y_start_pred, y_end_pred = model(x)\n            optimizer.zero_grad()\n            loss = loss_fn(y_start_pred, y_end_pred, y_start, y_end)\n            loss.backward()\n            optimizer.step()\n            sum_loss += loss.item()*y_start.shape[0]\n            total += y_start.shape[0]\n        val_loss, val_jacc = val_metrics(model, val_dl)\n        print(\"train loss %.3f val loss %.3f and jaccard score %.3f\" % (sum_loss/total, val_loss, val_jacc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def val_metrics(model, valid_dl):\n    model.eval()\n    total = 0\n    sum_loss = 0.0\n    jacc_score_list = []\n    for x, y_start, y_end in valid_dl:\n#         print(x)\n#         print(y_start)\n        x = x.long().cuda()\n        y_start = y_start.float().cuda()\n        y_end = y_end.float().cuda()\n        y_start_pred, y_end_pred = model(x)\n        loss = loss_fn(y_start_pred, y_end_pred, y_start, y_end)\n        sum_loss += loss.item()*y_start.shape[0]\n        \n        \n        \n        for i in range(len(x)):\n            tmp = list(X_val[total+i])\n            y_start_pred = y_start_pred\n            y_end_pred = y_end_pred\n            pred_str = \" \".join(list(tmp[max(0, int(y_start_pred[i])):max(0, int(y_end_pred[i]))]))\n#             print(y_start[i].detach().cpu().numpy())\n#             print(y_end[i].detach().cpu().numpy())\n#             print(tmp)\n#             print(tmp[y_start[i].detach().cpu().numpy():y_end[i]].detach().cpu().numpy())\n            selected_text = \" \".join(tmp[int(y_start[i].detach().cpu().numpy()) : int(y_end[i].detach().cpu().numpy())])\n            jacc_score = jaccard(pred_str, selected_text)\n            jacc_score_list.append(jacc_score)\n        \n        total += y_start.shape[0]\n    jacc_mean = np.mean(jacc_score_list)\n        \n    return sum_loss/total, jacc_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(words)\nprint(vocab_size)\nmodel = TweetGRU(vocab_size, 500, 500).cuda()\n\nparameters = filter(lambda p: p.requires_grad, model.parameters())\noptimizer = torch.optim.Adam(parameters, lr=0.001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_epocs(model, optimizer, train_dl, val_dl, epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"update_optimizer(optimizer, lr=0.001)\ntrain_epocs(model, optimizer, train_dl, val_dl, epochs=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}